{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Workflow Pipeline for Remote Sensing Data\n",
    "\n",
    "This notebook demonstrates how to create an automated workflow pipeline for processing remote sensing data using `snakemake` and Python. The pipeline integrates downloading Sentinel-2 imagery, preprocessing (e.g., cropping, reprojecting), cloud detection (using a pre-trained model from `28_cloud_detection_deep_learning.ipynb`), and calculating vegetation indices (e.g., NDVI). This is useful for scalable, reproducible data processing.\n",
    "\n",
    "## Prerequisites\n",
    "- Install required libraries: `snakemake`, `sentinelhub`, `rasterio`, `geopandas`, `numpy`, `torch`, `matplotlib` (listed in `requirements.txt`).\n",
    "- A configuration file for SentinelHub (e.g., `sentinelhub_config.json`).\n",
    "- A GeoJSON or shapefile defining the area of interest (AOI) (e.g., `aoi.geojson`).\n",
    "- A pre-trained cloud detection model (e.g., from `28_cloud_detection_deep_learning.ipynb`).\n",
    "- Replace file paths with your own data.\n",
    "\n",
    "## Learning Objectives\n",
    "- Design a `snakemake` pipeline for automated remote sensing data processing.\n",
    "- Integrate data download, preprocessing, cloud detection, and analysis steps.\n",
    "- Execute the pipeline and validate outputs.\n",
    "- Visualize processed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.mask import mask\n",
    "from sentinelhub import SHConfig, SentinelHubRequest, DataCollection, MimeType, CRS, BBox\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Configuration and Directories\n",
    "\n",
    "Define paths, SentinelHub credentials, and create necessary directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and configuration\n",
    "config_path = 'sentinelhub_config.json'  # Replace with your SentinelHub config file\n",
    "aoi_path = 'aoi.geojson'                # Replace with your AOI file\n",
    "output_dir = 'remote_sensing_data/processed/'\n",
    "cloud_model_path = 'unet_cloud_detection.pth'  # From 28_cloud_detection_deep_learning.ipynb\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load SentinelHub configuration\n",
    "with open(config_path, 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "config = SHConfig()\n",
    "config.instance_id = config_dict['instance_id']\n",
    "config.sh_client_id = config_dict['client_id']\n",
    "config.sh_client_secret = config_dict['client_secret']\n",
    "\n",
    "# Load AOI\n",
    "aoi_gdf = gpd.read_file(aoi_path)\n",
    "aoi_bounds = aoi_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "aoi_bbox = BBox(bbox=aoi_bounds, crs=CRS.WGS84)\n",
    "\n",
    "print(f'AOI bounds: {aoi_bounds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Snakemake Pipeline\n",
    "\n",
    "Create a `Snakemake` workflow to automate data download, preprocessing, cloud detection, and NDVI calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "configfile: 'config.yaml'\n",
    "AOI_PATH = config['aoi_path']\n",
    "OUTPUT_DIR = config['output_dir']\n",
    "START_DATE = config['start_date']\n",
    "END_DATE = config['end_date']\n",
    "DATES = [datetime.strptime(START_DATE, '%Y-%m-%d').strftime('%Y%m%d'),\n",
    "         datetime.strptime(END_DATE, '%Y-%m-%d').strftime('%Y%m%d')]\n",
    "\n",
    "# Rules\n",
    "rule all:\n",
    "    input:\n",
    "        expand('{output_dir}/ndvi_{date}.tif', output_dir=OUTPUT_DIR, date=DATES)\n",
    "\n",
    "rule download_sentinel2:\n",
    "    output:\n",
    "        '{output_dir}/sentinel2_{date}.tif'\n",
    "    run:\n",
    "        from sentinelhub import SentinelHubRequest, DataCollection, MimeType, CRS, BBox\n",
    "        import geopandas as gpd\n",
    "        import rasterio\n",
    "        from rasterio.crs import CRS as rioCRS\n",
    "\n",
    "        aoi_gdf = gpd.read_file(AOI_PATH)\n",
    "        aoi_bounds = aoi_gdf.total_bounds\n",
    "        aoi_bbox = BBox(bbox=aoi_bounds, crs=CRS.WGS84)\n",
    "\n",
    "        evalscript = '''\n",
    "        //VERSION=3\n",
    "        function setup() {{\n",
    "            return {{\n",
    "                input: [\"B04\", \"B08\", \"B03\", \"B02\"],\n",
    "                output: {{ bands: 4, sampleType: \"FLOAT32\" }}\n",
    "            }};\n",
    "        }}\n",
    "\n",
    "        function evaluatePixel(sample) {{\n",
    "            return [sample.B04, sample.B08, sample.B03, sample.B02];\n",
    "        }}\n",
    "        '''\n",
    "\n",
    "        date = wildcards.date\n",
    "        request = SentinelHubRequest(\n",
    "            evalscript=evalscript,\n",
    "            input_data=[SentinelHubRequest.input_data(\n",
    "                data_collection=DataCollection.SENTINEL2_L1C,\n",
    "                time_interval=(f'{date[:4]}-{date[4:6]}-{date[6:]}', f'{date[:4]}-{date[4:6]}-{date[6:]}')\n",
    "            )],\n",
    "            responses=[SentinelHubRequest.output_response('default', MimeType.TIFF)],\n",
    "            bbox=aoi_bbox,\n",
    "            size=(512, 512),\n",
    "            config=config\n",
    "        )\n",
    "        data = request.get_data()[0]\n",
    "        with rasterio.open(output[0], 'w', driver='GTiff', height=data.shape[0], width=data.shape[1],\n",
    "                           count=4, dtype='float32', crs=rioCRS.from_epsg(4326), transform=request.get_transform()) as dst:\n",
    "            dst.write(data.transpose(2, 0, 1))\n",
    "\n",
    "rule preprocess_raster:\n",
    "    input:\n",
    "        raster='{output_dir}/sentinel2_{date}.tif',\n",
    "        aoi=AOI_PATH\n",
    "    output:\n",
    "        '{output_dir}/preprocessed_{date}.tif'\n",
    "    run:\n",
    "        import rasterio\n",
    "        from rasterio.mask import mask\n",
    "        import geopandas as gpd\n",
    "        from rasterio.warp import reproject, Resampling\n",
    "\n",
    "        aoi_gdf = gpd.read_file(input.aoi)\n",
    "        with rasterio.open(input.raster) as src:\n",
    "            raster_data, transform = mask(src, aoi_gdf.geometry, crop=True, nodata=np.nan)\n",
    "            profile = src.profile\n",
    "            profile.update({\n",
    "                'height': raster_data.shape[1],\n",
    "                'width': raster_data.shape[2],\n",
    "                'transform': transform,\n",
    "                'nodata': np.nan,\n",
    "                'crs': 'EPSG:32632'  # Example UTM zone, adjust as needed\n",
    "            })\n",
    "\n",
    "        # Reproject to UTM\n",
    "        with rasterio.open(output[0], 'w', **profile) as dst:\n",
    "            dst.write(raster_data)\n",
    "\n",
    "rule cloud_detection:\n",
    "    input:\n",
    "        raster='{output_dir}/preprocessed_{date}.tif'\n",
    "    output:\n",
    "        '{output_dir}/cloud_mask_{date}.tif'\n",
    "    run:\n",
    "        import rasterio\n",
    "        import torch\n",
    "        import segmentation_models_pytorch as smp\n",
    "        import numpy as np\n",
    "\n",
    "        model = smp.Unet(encoder_name='resnet18', in_channels=4, classes=2).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.load_state_dict(torch.load('unet_cloud_detection.pth', map_location='cpu'))\n",
    "        model.eval()\n",
    "\n",
    "        with rasterio.open(input.raster) as src:\n",
    "            raster_data = src.read(masked=True)\n",
    "            profile = src.profile\n",
    "\n",
    "        norm_data = raster_data / np.nanpercentile(raster_data, 98, axis=(1, 2), keepdims=True)\n",
    "        norm_data = np.clip(norm_data, 0, 1)\n",
    "\n",
    "        patch_size = 256\n",
    "        height, width = norm_data.shape[1], norm_data.shape[2]\n",
    "        cloud_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, height - patch_size + 1, patch_size//2):\n",
    "                for j in range(0, width - patch_size + 1, patch_size//2):\n",
    "                    patch = norm_data[:, i:i+patch_size, j:j+patch_size]\n",
    "                    if not np.any(np.isnan(patch)):\n",
    "                        patch_tensor = torch.from_numpy(patch.astype(np.float32)).unsqueeze(0).to(model.device)\n",
    "                        output = torch.sigmoid(model(patch_tensor)).cpu().numpy()\n",
    "                        pred = (output[0, 0] > 0.5).astype(np.uint8)\n",
    "                        cloud_mask[i:i+patch_size, j:j+patch_size] = pred\n",
    "\n",
    "        profile.update({'count': 1, 'dtype': 'uint8', 'nodata': None})\n",
    "        with rasterio.open(output[0], 'w', **profile) as dst:\n",
    "            dst.write(cloud_mask, 1)\n",
    "\n",
    "rule calculate_ndvi:\n",
    "    input:\n",
    "        raster='{output_dir}/preprocessed_{date}.tif',\n",
    "        cloud_mask='{output_dir}/cloud_mask_{date}.tif'\n",
    "    output:\n",
    "        '{output_dir}/ndvi_{date}.tif'\n",
    "    run:\n",
    "        import rasterio\n",
    "        import numpy as np\n",
    "\n",
    "        with rasterio.open(input.raster) as src:\n",
    "            raster_data = src.read(masked=True)\n",
    "            profile = src.profile\n",
    "        with rasterio.open(input.cloud_mask) as src:\n",
    "            cloud_mask = src.read(1, masked=True)\n",
    "\n",
    "        red = raster_data[0].astype(float)\n",
    "        nir = raster_data[1].astype(float)\n",
    "        ndvi = np.where((nir + red) != 0, (nir - red) / (nir + red), np.nan)\n",
    "        ndvi[cloud_mask == 1] = np.nan\n",
    "\n",
    "        profile.update({'count': 1, 'dtype': 'float32'})\n",
    "        with rasterio.open(output[0], 'w', **profile) as dst:\n",
    "            dst.write(ndvi, 1)\n",
    "\n",
    "# Configuration file\n",
    "%%writefile config.yaml\n",
    "aoi_path: 'aoi.geojson'\n",
    "output_dir: 'remote_sensing_data/processed'\n",
    "start_date: '2023-01-01'\n",
    "end_date: '2023-12-31'\n",
    "cloud_model_path: 'unet_cloud_detection.pth'\n",
    "\n",
    "# Run pipeline (in terminal: `snakemake -c1`)\n",
    "print('Snakemake pipeline defined. Run `snakemake -c1` in the terminal to execute.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Execute Pipeline\n",
    "\n",
    "Run the `snakemake` pipeline to process the data. This step is typically executed in the terminal, but a sample execution is shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute pipeline programmatically (for demonstration)\n",
    "!snakemake -c1 --quiet\n",
    "\n",
    "# List output files\n",
    "output_files = glob.glob(os.path.join(output_dir, 'ndvi_*.tif'))\n",
    "print(f'Generated NDVI files: {output_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Results\n",
    "\n",
    "Visualize the NDVI output for the first processed date with AOI overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize NDVI\n",
    "if output_files:\n",
    "    with rasterio.open(output_files[0]) as src:\n",
    "        ndvi_data = src.read(1, masked=True)\n",
    "        ndvi_profile = src.profile\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(ndvi_data, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "    aoi_gdf.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=2)\n",
    "    plt.colorbar(ax=ax, label='NDVI')\n",
    "    plt.title(f'NDVI - {os.path.basename(output_files[0]).split(\"_\")[1][:-4]}')\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Row')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No NDVI files generated. Check pipeline execution.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Replace `sentinelhub_config.json`, `aoi.geojson`, and `unet_cloud_detection.pth` with your own files.\n",
    "- Update `config.yaml` with desired dates and output directory.\n",
    "- Extend the pipeline by adding rules for other analyses (e.g., classification from `12_classification_rf_svm.ipynb` or change detection from `30_change_detection_timeseries.ipynb`).\n",
    "- Use outputs in visualization notebooks like `23_kepler_gl_demo.ipynb` or `26_time_series_animation.ipynb`.\n",
    "- Explore advanced `snakemake` features like parallel execution or cloud integration.\n",
    "\n",
    "## Notes\n",
    "- Ensure SentinelHub credentials are valid in `sentinelhub_config.json`.\n",
    "- Adjust the UTM zone in the preprocessing rule based on your AOI.\n",
    "- The cloud detection model assumes a pre-trained U-Net from `28_cloud_detection_deep_learning.ipynb`.\n",
    "- Run `snakemake -c1` in the terminal for sequential execution or `-cN` for parallel processing (N=number of cores).\n",
    "- See `docs/installation.md` for troubleshooting library installation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}